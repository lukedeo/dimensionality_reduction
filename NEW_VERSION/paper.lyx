#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass report
\begin_preamble
\usepackage{lettrine}
\newcommand{\R}{\mathbb{R}}
\end_preamble
\use_default_options true
\begin_modules
eqs-within-sections
theorems-std
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family rmdefault
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement H
\paperfontsize default
\spacing double
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1.2in
\topmargin 1.2in
\rightmargin 1.2in
\bottommargin 1.4in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
DimReduce -- an R package for dimensionality reduction
\end_layout

\begin_layout Author
Luke de Oliveira
\end_layout

\begin_layout Abstract
\begin_inset FormulaMacro
\newcommand{\R}{\mathbb{R}}
{\mathbb{R}}
\end_inset


\end_layout

\begin_layout Abstract
In this short exposition, we show the various methods implemented in DimReduce,
 taking care to show where steps have been taken to increase efficiency
 of computation.
 In addition, we utilize select example datasets to show some strengths
 and weaknesses of various dimensionality reduction algorithms.
 
\end_layout

\begin_layout Chapter
Background
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
lettrine[nindent=0em,lines=2]{M}
\end_layout

\end_inset

 any problems in areas ranging from data visualization to pattern recognition
 begin with a preprocessing step.
 Frequently, it is desirable to reduce the dimensionality of a dataset --
 that is, find a representation in a lower dimensional space that accurately
 represents some aspect of the dataset, be it local structure, global structure,
 et cetera.
 
\end_layout

\begin_layout Standard
Many applications of Machine Learning have some high dimensional dataset
 that lies on some lower dimensional space -- that is, there some level
 of redundancy of information.
 The general problem can be formulated as follows.
 We are given a dataset 
\begin_inset ERT
status open

\begin_layout Plain Layout

$
\backslash
{x_i
\backslash
}_{i=1}^{N}$
\end_layout

\end_inset

, where each 
\begin_inset ERT
status open

\begin_layout Plain Layout

$x_i
\backslash
in
\backslash
R^D$
\end_layout

\end_inset

.
 The problem is to find an embedding 
\begin_inset ERT
status open

\begin_layout Plain Layout

$
\backslash
{y_i
\backslash
}_{i=1}^{N}$
\end_layout

\end_inset

, where each 
\begin_inset ERT
status open

\begin_layout Plain Layout

$y_i
\backslash
in
\backslash
R^d$
\end_layout

\end_inset

, 
\begin_inset ERT
status open

\begin_layout Plain Layout

$d<D$
\end_layout

\end_inset

, such that 
\begin_inset Formula $\{x_{i}\}_{i=1}^{N}$
\end_inset

 is "represented well" by 
\begin_inset Formula $\{y_{i}\}_{i=1}^{N}$
\end_inset

.
 The notion of "represented well" is far from concrete, but many metrics
 for the quality of an embedding exist.
 
\end_layout

\begin_layout Standard
In addition to the dataset, we assume we have access to a distance matrix,
 
\begin_inset ERT
status open

\begin_layout Plain Layout

$
\backslash
Delta
\backslash
in
\backslash
R^{N
\backslash
times N}$
\end_layout

\end_inset

.
 That is, 
\begin_inset Formula $\Delta_{i,j}$
\end_inset

 is the distance from 
\begin_inset Formula $x_{i}$
\end_inset

 to 
\begin_inset Formula $x_{j}$
\end_inset

.
 For an arbitrary matrix 
\begin_inset Formula $A$
\end_inset

, I use numpy-style slicing notation, i.e., 
\begin_inset Formula $A[v,:]$
\end_inset

 chooses the rows corresponding to some vector of indices 
\begin_inset Formula $v$
\end_inset

.
\end_layout

\begin_layout Standard
In terms of notation, for two matricies 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

, 
\begin_inset Formula $A\odot B$
\end_inset

 represents the Hadamard Product.
 For a matrix 
\begin_inset Formula $A$
\end_inset

, 
\begin_inset Formula $\mathsf{sum}(A)=\sum_{i}\sum_{j}[A]_{ij}$
\end_inset

.
\end_layout

\begin_layout Section
Implementation
\end_layout

\begin_layout Standard
The crux of all the dimensionality reduction algorithms we provide is efficiency
 linear algebra.
 Instead of using the base R implementation of matrix mathematics, we utilize
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{RcppArmadillo}
\end_layout

\end_inset

, an extension of 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{Rcpp}
\end_layout

\end_inset

.
 The package 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{Rcpp}
\end_layout

\end_inset

 provides an easy interface between fast C++ code and R, allowing a user
 to extend R by calling functions and methods that are more efficiently
 implemented in C++.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{RcppArmadillo}
\end_layout

\end_inset

 takes this a step further, by allowing the R developer the ability to call
 the Armadillo Library -- a very efficient matrix library for C++.
 Using this and a deeper understanding of how to efficiently implement methods,
 we obtain speed-ups over existing packages that provide methods that we
 include in DimReduce.
\end_layout

\begin_layout Standard
In addition to methods for dimensionality reduction, we provide a method
 for fast pairwise distances -- 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{fastPdist}
\end_layout

\end_inset

 -- which is significantly faster than the native R function.
 We also provide a fast method for determining nearest neighbors from a
 matrix 
\begin_inset ERT
status open

\begin_layout Plain Layout

$
\backslash
Delta$
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
To determine speed-up, based on a baseline, we run a test using an INSERT
 MACHARCH HERE.
 As a test, we use a swiss roll with 
\begin_inset ERT
status open

\begin_layout Plain Layout

$N=150$, $D=3$, $d = 2$
\end_layout

\end_inset

, and 
\begin_inset ERT
status open

\begin_layout Plain Layout

$k=5$
\end_layout

\end_inset

 with 
\begin_inset ERT
status open

\begin_layout Plain Layout

$n_{iter} = 200$ 
\end_layout

\end_inset

when appropriate.
 
\end_layout

\begin_layout Section
Methods
\end_layout

\begin_layout Standard
We don't provide proofs of optimal solutions to these methods -- instead,
 we provide a basic walk-through of the algorithm, and state where efficiency
 improvements were made.
 We provide efficient implementations of the following methods: 
\end_layout

\begin_layout Itemize
Random Projection 
\end_layout

\begin_layout Itemize
Classical Multidimensional Scaling 
\end_layout

\begin_layout Itemize
Locally Linear Embedding (Modified and Hessian)
\end_layout

\begin_layout Itemize
Isomap 
\end_layout

\begin_layout Itemize
Laplacian Eigenmaps 
\end_layout

\begin_layout Itemize
Diffusion Maps 
\end_layout

\begin_layout Itemize
Local Multidimensional Scaling using the boxcox family of stress functions
 
\end_layout

\begin_layout Standard
For each method, we benchmark qualitative performance using three datasets.
 First, we use the standard Swiss Roll and S-Curve data sets.
 In addition to these artificial manifolds, we also use the Frey Faces data
 set from CITATION.
 In the Frey Faces dataset, embeddings that show similar colors along a
 rainbow spectrum indicate that images are similar to each other -- i.e.,
 they may represent a slight rotation of the head.
\end_layout

\begin_layout Standard

\end_layout

\begin_layout Chapter
Algorithms
\end_layout

\begin_layout Section
Random Projection
\end_layout

\begin_layout Subsection
Description and Implementation
\end_layout

\begin_layout Standard
A random projection is a simple linear mapping that projects the original
 dataset into a random subspace of a user-determined dimension.
 Random projections rely on the notion of a Lipschitz Mapping.
 
\end_layout

\begin_layout Definition
Let 
\begin_inset Formula $U\subset\R^{D}$
\end_inset

.
 A mapping 
\begin_inset Formula $f:U\longrightarrow\R^{d}$
\end_inset

 is called Lipschitz on 
\begin_inset Formula $U$
\end_inset

 if 
\begin_inset Formula $\exists\alpha,\beta\in\R$
\end_inset

 such that 
\begin_inset Formula 
\[
\alpha\Vert x-y\Vert^{2}\leq\Vert f(x)-f(y)\Vert^{2}\leq\beta\Vert x-y\Vert^{2}.
\]

\end_inset


\end_layout

\begin_layout Definition
In this case, 
\begin_inset Formula $\Vert\cdot\Vert$
\end_inset

 is the Euclidean norm, but can be replaced by any norm.
 
\end_layout

\begin_layout Standard
How can this help with reducing the dimensionality of a dataset? We turn
 to Johnson and Lindenstrauss CITATION, who provide the following Lemma,
 which we provide without proof.
\end_layout

\begin_layout Lemma
(Johnson and Lindenstrauss) Fix 
\begin_inset Formula $\varepsilon>0,n\in\mathbb{N}$
\end_inset

.
 Let 
\begin_inset Formula $d_{0}\le O(\varepsilon^{-2}\log n)$
\end_inset

.
 Then, 
\begin_inset Formula $\forall d\geq d_{0}$
\end_inset

 and for any randomly selected 
\begin_inset Formula $U\subset\mathbb{R}^{D}$
\end_inset

 such that 
\begin_inset Formula $|U|=n$
\end_inset

, 
\begin_inset Formula $\exists f:\R^{D}\longrightarrow\R^{d}$
\end_inset

 such that 
\begin_inset Formula $\forall x,y\in U$
\end_inset

,
\begin_inset Formula 
\begin{equation}
(1-\varepsilon)\Vert x-y\Vert^{2}\leq\Vert f(x)-f(y)\Vert^{2}\leq(1+\varepsilon)\Vert x-y\Vert^{2}.\label{eq:jlindenstrauss}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Intuitively REFABOVE tells us that we can find approximate distance preserving
 mappings that embed a data point in a lower dimensional space.
 In fact, X and X show in XX that we can construct matricies that have zero
 mean and unit variance that in expectation provide a pairwise distance
 preserving embedding.
 In particular, we it can be shown that we can construct a gaussian matrix
 
\begin_inset Formula $R$
\end_inset

 with 
\begin_inset Formula $R_{ij}\sim N(0,1)$
\end_inset

 such that in expectation, a normalized version of 
\begin_inset Formula $R$
\end_inset

 is a good mapping from 
\begin_inset Formula $D$
\end_inset

 to 
\begin_inset Formula $d$
\end_inset

.
 More formally, we can describe this simple algorithm as follows.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
Input: 
\begin_inset Formula $X\in\R^{N\times D}$
\end_inset

, 
\begin_inset Formula $d$
\end_inset


\end_layout

\begin_layout Plain Layout
Output: 
\begin_inset Formula $Y\in\R^{N\times d}$
\end_inset


\end_layout

\begin_layout Enumerate
Let 
\begin_inset Formula $R\in\R^{D\times d}$
\end_inset

, with 
\begin_inset Formula $R_{ij}\sim N(0,1)$
\end_inset


\end_layout

\begin_layout Enumerate
Let 
\begin_inset Formula $Y=\frac{1}{\sqrt{k}}XR$
\end_inset


\end_layout

\begin_layout Enumerate
Return: 
\begin_inset Formula $Y$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Random Projection
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset

Imagine a scenario in which we find such an 
\begin_inset Formula $R$
\end_inset

 that gives a desirable embedding.
 This method is very appealing because embedding new points that weren't
 in the original 
\begin_inset Formula $X$
\end_inset

 can be embedded in 
\begin_inset Formula $O(dD)$
\end_inset

 time.
 
\end_layout

\begin_layout Section
Metric 
\begin_inset CommandInset label
LatexCommand label
name "sec:Metric-Multidimensional-Scaling"

\end_inset

Multidimensional Scaling
\end_layout

\begin_layout Subsection
Description and Implementation
\end_layout

\begin_layout Standard
Classical Multidimensional Scaling aims to approximately solve the problem
 of Equation 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:jlindenstrauss"

\end_inset

 by explicitly minimizing a cost function that penalized points whose embedded
 representations aren't close in euclidean distance.
 Formally, given a matrix of distances 
\begin_inset Formula $\Delta$
\end_inset

 with entries 
\begin_inset Formula $\Delta_{ij}$
\end_inset

, we let our embedding be 
\begin_inset Formula $\{y_{i}\}_{i=1}^{N}$
\end_inset

 with 
\begin_inset Formula $y_{i}\in\R^{d}$
\end_inset

, and we let
\begin_inset Formula 
\[
\{y_{i}\}_{i=1}^{N}=\underset{\{a_{i}\}_{i=1}^{N}\subset\R^{d}}{\text{arg min}}\sum_{i=1}^{n}\sum_{j\neq i}(\Delta_{ij}-\Vert a_{i}-a_{j}\Vert)^{2}.
\]

\end_inset


\end_layout

\begin_layout Standard
We provide without proof the decomposition that leads to an optimal embedding.
 Let 
\begin_inset Formula $J=I-\frac{1}{N}\mathbf{1}_{\{N\times N\}}$
\end_inset

, where 
\begin_inset Formula $\mathbf{1}_{\{N\times N\}}$
\end_inset

 is the 
\begin_inset Formula $N\times N$
\end_inset

 dimensional matrix of all ones.
 We let out Kernel be 
\begin_inset Formula $K=-\frac{1}{2}J\{\Delta\}^{2}J$
\end_inset

, where 
\begin_inset Formula $\{\Delta\}^{2}$
\end_inset

 is each element of the matrix squared, rather than a matrix power.
 We then perform eigendecomposition 
\begin_inset Formula $K=V\Lambda V^{-1}$
\end_inset

, and let 
\begin_inset Formula $\{y_{i}\}_{i=1}^{N}=Y\in\R^{N\times d}$
\end_inset

 be the columns of the eigenvectors 
\begin_inset Formula $V$
\end_inset

 associated with the two largest eigenvalues.
 To reiterate, we have that Classical Multidimensional Scaling can be written
 as follows.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
Input: A distance matrix 
\begin_inset Formula $\Delta\in\R^{N\times N}$
\end_inset

, an embedding dimension 
\begin_inset Formula $d$
\end_inset

.
\end_layout

\begin_layout Plain Layout
Output: An embedding 
\begin_inset Formula $\{y_{i}\}_{i=1}^{N}=Y\in\R^{N\times d}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Let 
\begin_inset Formula $J=I-\frac{1}{N}\mathbf{1}_{\{N\times N\}}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Construct the kernel 
\begin_inset Formula $K=-\frac{1}{2}J\{\Delta\}^{2}J$
\end_inset

.
\end_layout

\begin_layout Enumerate
Decompose into 
\begin_inset Formula $K=V\Lambda V^{-1}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Sort columns of 
\begin_inset Formula $V$
\end_inset

 in decreasing order based on the associated eigenvalues.
\end_layout

\begin_layout Enumerate
Let 
\begin_inset Formula $Y=V[:,1:d]$
\end_inset

.
\end_layout

\begin_layout Enumerate
Return 
\begin_inset Formula $Y$
\end_inset

.
\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Classical Multidimensional Scaling
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
It should be noted that in the case of the Euclidean Metric, Principal Component
s Analysis (PCA) and Classical Multidimensional Scaling (CMDS) are the same.
 However, the motivations and data models of PCA and CMDS differ -- In PCA,
 a data set 
\begin_inset Formula $X$
\end_inset

 is given, and PCA aims to find its leading 
\begin_inset Formula $d$
\end_inset

 principal components that preserve the maximal variance.
 In CMDS, a pairwise distance matrix is given on a set of 
\begin_inset Formula $n$
\end_inset

 objects and the aim is to find a lower dimensional embedding or point set
 in a Euclidean space that preserves the pairwise (dis)similarities.
 However, if the set of distances in CMDS is used to construct a 
\begin_inset Formula $D$
\end_inset

-dimensional set 
\begin_inset Formula $X$
\end_inset

 in 
\begin_inset Formula $\R^{D}$
\end_inset

, and the pairwise distance matrix is the Euclidean metric of 
\begin_inset Formula $X$
\end_inset

 , then PCA and CMDS are identical.
\end_layout

\begin_layout Subsection
Experiments
\end_layout

\begin_layout Subsubsection
Swiss Roll
\end_layout

\begin_layout Standard
Consider the following embeddings.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
CMDS of Swiss Roll Dataset, 
\begin_inset Formula $N=1000$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
S-Curve
\end_layout

\begin_layout Standard
Consider the following embeddings.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
CMDS of S-Curve Dataset, 
\begin_inset Formula $N=1000$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Frey Faces
\end_layout

\begin_layout Standard
Consider the following embeddings.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
CMDS of Frey Faces Dataset, 
\begin_inset Formula $N=1000$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
OBSERVATIONS HERE.
\end_layout

\begin_layout Section
Locally Linear Embedding
\end_layout

\begin_layout Subsection
Description
\end_layout

\begin_layout Standard
Locally Linear Embedding is a very intuitive method which attempts to find
 an embedding that preserves local neighborhood structure of the data.
 Formally, the algorithm assumes that each point in the dataset can be described
 as a convex combination of nearby data points.
 If a data set is densely sampled from a smooth manifold, then neighbors
 of each point remain nearby and are similarly located in the assumed low-dimens
ional space.
 In Locally Linear Embedding, each point in the data set is linearly embedded
 into a locally linear subspace on the manifold.
 Then, using eigendecomposition, the method tries to find a new embedding
 for each point such that the new embedding is similar in locally convex
 reconstruction to the original dataset.
 It should be noted that in any large scale implementation of Locally Linear
 Embedding, the inherent sparsity of the reconstruction kernel can be exploited.
 In this version, we do not exploit the sparsity of the kernel.
 
\end_layout

\begin_layout Subsection
Implementation
\end_layout

\begin_layout Standard
The Locally Linear Embedding algorithm can be outlined as follows.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
Input: 
\begin_inset Formula $X\in\R^{N\times D}$
\end_inset

, 
\begin_inset Formula $k$
\end_inset

, 
\begin_inset Formula $d$
\end_inset


\end_layout

\begin_layout Plain Layout
Output: 
\begin_inset Formula $Y\in\R^{N\times d}$
\end_inset


\end_layout

\begin_layout Enumerate
Let 
\begin_inset Formula $A\in\{0,1\}^{N\times N}$
\end_inset

 be the 
\begin_inset Formula $k$
\end_inset

 nearest neighbor matrix
\end_layout

\begin_layout Enumerate
Let 
\begin_inset Formula $W\in\R^{N\times N}$
\end_inset

 be an empty weight matrix
\end_layout

\begin_layout Enumerate
For each 
\begin_inset Formula $i\in\{1,\ldots,N\}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset Formula $Z=X[A[i,:],:]^{T}$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $Z=Z-(\mathbb{1}_{k}X[i,:])^{T}$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $C=Z^{T}Z$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $C=C+0.001\mathrm{tr}(C)I$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $w=\frac{C^{-1}\mathbb{1}_{k}}{(C^{-1}\mathbb{1}_{k})^{T}\mathbb{1}_{k}}$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $W[i,A[i,:]]=-w^{T}$
\end_inset

 
\end_layout

\end_deeper
\begin_layout Enumerate
\begin_inset Formula $M=W+I$
\end_inset


\end_layout

\begin_layout Enumerate
Determine SVD, 
\begin_inset Formula $U,\Sigma,V$
\end_inset

, such that 
\begin_inset Formula $U\Sigma V^{T}=M$
\end_inset


\end_layout

\begin_layout Enumerate
Sort columns of 
\begin_inset Formula $V$
\end_inset

 based on increasing singular values
\end_layout

\begin_layout Enumerate
Return: 
\begin_inset Formula $Y=V[:,2:(d+1)]$
\end_inset

 
\begin_inset Caption

\begin_layout Plain Layout
Locally Linear Embedding
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The key difference when compared to other implementations in R is that we
 do not use an eigendecomposition on the matrix 
\begin_inset Formula $M^{T}M$
\end_inset

 -- we simply use the singular value decomposition, since the columns of
 the right singular vector matrix 
\begin_inset Formula $V$
\end_inset

 are the eigenvalues of the 
\begin_inset Formula $M^{T}M$
\end_inset

 matrix.
 To ensure the non-singularity of the reconstruction requirement matrix
 
\begin_inset Formula $C$
\end_inset

, we add 
\begin_inset Formula $\mathrm{tr}(C)\varepsilon I$
\end_inset

, for some small 
\begin_inset Formula $\varepsilon$
\end_inset

.
 Consider the following figure which shows a graphical representation of
 the speed-up obtained.
 The baseline considered for this method is the package 
\family typewriter
lle
\family default
.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:su_lle"

\end_inset

 
\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../Paper/speedup_lle.pdf
	lyxscale 10
	width 40text%

\end_inset

 
\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Ratio of CPU time for DimReduce versus a baseline
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Experiments
\end_layout

\begin_layout Standard
In the following experiments, we use 
\begin_inset Formula $N=1000$
\end_inset

 data points to ensure a reasonable sampled manifold.
 In addition, we scale all variables 
\begin_inset Formula $x$
\end_inset

 such that 
\begin_inset Formula $\overline{x}=0$
\end_inset

 and 
\begin_inset Formula $s_{x}=1$
\end_inset

 to ensure that no particular original feature has an imbalance of influence
 on the objective function.
 We use 
\begin_inset Formula $k\in\{5,10,15,20\}$
\end_inset

 to illustrate the effect of neighborhood size on the unrolled representation.
\end_layout

\begin_layout Subsubsection
Swiss Roll
\end_layout

\begin_layout Standard
Consider the following embeddings.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Locally Linear Embedding of Swiss Roll Dataset, 
\begin_inset Formula $k=5$
\end_inset

, 
\begin_inset Formula $N=1000$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Locally Linear Embedding of Swiss Roll Dataset, 
\begin_inset Formula $k=10$
\end_inset

, 
\begin_inset Formula $N=1000$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Locally Linear Embedding of Swiss Roll Dataset, 
\begin_inset Formula $k=15$
\end_inset

, 
\begin_inset Formula $N=1000$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Locally Linear Embedding of Swiss Roll Dataset, 
\begin_inset Formula $k=20$
\end_inset

, 
\begin_inset Formula $N=1000$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
S-Curve
\end_layout

\begin_layout Standard
Consider the following embeddings.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Locally Linear Embedding of S-Curve Dataset, 
\begin_inset Formula $k=5$
\end_inset

, 
\begin_inset Formula $N=1000$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Locally Linear Embedding of S-Curve Dataset, 
\begin_inset Formula $k=10$
\end_inset

, 
\begin_inset Formula $N=1000$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Locally Linear Embedding of S-Curve Dataset, 
\begin_inset Formula $k=15$
\end_inset

, 
\begin_inset Formula $N=1000$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Locally Linear Embedding of S-Curve Dataset, 
\begin_inset Formula $k=20$
\end_inset

, 
\begin_inset Formula $N=1000$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Frey Faces
\end_layout

\begin_layout Standard
Consider the following embeddings.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Locally Linear Embedding of Frey Faces Dataset, 
\begin_inset Formula $k=5$
\end_inset

, 
\begin_inset Formula $N=1000$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Locally Linear Embedding of Frey Faces Dataset, 
\begin_inset Formula $k=10$
\end_inset

, 
\begin_inset Formula $N=1000$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Locally Linear Embedding of Frey Faces Dataset, 
\begin_inset Formula $k=15$
\end_inset

, 
\begin_inset Formula $N=1000$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Locally Linear Embedding of Frey Faces Dataset, 
\begin_inset Formula $k=20$
\end_inset

, 
\begin_inset Formula $N=1000$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Isomap 
\end_layout

\begin_layout Subsection
Description
\end_layout

\begin_layout Standard
Isomap is essentially Multidimensional Scaling with respect to a different
 baseline metric.
 Instead of using the standard Euclidean Metric, we first construct a graph
 on the dataset.
 We let each 
\begin_inset Formula $x_{i}\in\{x_{i}\}_{i=1}^{N}$
\end_inset

 define a vertex, and we connect vertex 
\begin_inset Formula $x_{i}$
\end_inset

 and vertex 
\begin_inset Formula $x_{j}$
\end_inset

 if 
\begin_inset Formula $i$
\end_inset

 and 
\begin_inset Formula $j$
\end_inset

 are in the same neighborhood.
 We can define a neighborhood either by 
\begin_inset Formula $k$
\end_inset

 nearest neighbors or an 
\begin_inset Formula $\varepsilon$
\end_inset

-neighborhood.
 In general, a 
\begin_inset Formula $k$
\end_inset

 nearest neighbor graph is used more in practice as an 
\begin_inset Formula $\varepsilon$
\end_inset

-neighborhood graph requires an a priori knowledge of distances.
 When we connect all vertices in neighborhoods, we have a set of edges 
\begin_inset Formula $E$
\end_inset

, and we have an weighted graph
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
We assume that 
\begin_inset Formula $G$
\end_inset

 is connected.
 In practice, if 
\begin_inset Formula $G$
\end_inset

 is not connected, we can impute distances as necessary, or increase the
 neighborhood size.
\end_layout

\end_inset

 
\begin_inset Formula $G=(\{x_{i}\}_{i=1}^{N},E,\delta)$
\end_inset

, with our weights 
\begin_inset Formula $\delta$
\end_inset

 being an original euclidean pairwise distance.
 For an edge 
\begin_inset Formula $(x_{i},x_{j})=e\in E$
\end_inset

, let 
\begin_inset Formula $\delta(e)=\Vert x_{i}-x_{j}\Vert$
\end_inset

.
 For each pair, we determine the 
\emph on
geodesic
\emph default
 distance along the neighborhood-sampled manifold.
 That is, we find 
\begin_inset Formula $\textsf{dist}_{G}(x_{i},x_{j})$
\end_inset

 for each 
\begin_inset Formula $i,j$
\end_inset

 pair, with
\begin_inset Formula 
\[
\textsf{dist}_{G}(x_{i},x_{j})=\min_{\text{paths }P\text{ starting at \ensuremath{x_{i}}ending at \ensuremath{x_{j}}}}\sum_{e\in P}\delta(e).
\]

\end_inset


\end_layout

\begin_layout Standard
Our optimization problem is similar to that of Classical Multidimensional
 Scaling.
 We solve, for 
\begin_inset Formula $G=(\{x_{i}\}_{i=1}^{N},E,\delta)$
\end_inset

,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\{y_{i}\}_{i=1}^{N}=\underset{\{a_{i}\}_{i=1}^{N}\subset\R^{d}}{\text{arg min}}\sum_{i=1}^{n}\sum_{j\neq i}(\textsf{dist}_{G}(x_{i},x_{j})-\Vert a_{i}-a_{j}\Vert)^{2}.\label{eq:isomap}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
Implementation
\end_layout

\begin_layout Standard
To perform the optimization in Equation 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:isomap"

\end_inset

, we must be able to determine geodesic distances.
 To do this, we use the R package 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{igraph}
\end_layout

\end_inset

, which provides a fast implementation of Dijkstra's Algorithm for finding
 pairwise distances on a graph.
 With this routine (call it 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{dijkstra}
\end_layout

\end_inset

 for the sake of reading), we have the following algorithm.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
Input: 
\begin_inset Formula $X\in\R^{N\times D}$
\end_inset

, 
\begin_inset Formula $k$
\end_inset

, 
\begin_inset Formula $d$
\end_inset


\end_layout

\begin_layout Plain Layout
Output: 
\begin_inset Formula $Y\in\R^{N\times d}$
\end_inset


\end_layout

\begin_layout Enumerate
Compute 
\begin_inset Formula $\Delta$
\end_inset

, the pairwise Euclidean distance matrix on 
\begin_inset Formula $X$
\end_inset

.
\end_layout

\begin_layout Enumerate
Let 
\begin_inset Formula $A\in\{0,1\}^{N\times N}$
\end_inset

 be the 
\begin_inset Formula $k$
\end_inset

 nearest neighbor matrix.
\end_layout

\begin_layout Enumerate
Let 
\begin_inset Formula $G=(\{x_{i}\}_{i=1}^{N},E,\delta)$
\end_inset

 according to 
\begin_inset Formula $\Delta$
\end_inset

 and 
\begin_inset Formula $A$
\end_inset

.
\end_layout

\begin_layout Enumerate
Use 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{dijkstra}
\end_layout

\end_inset

 to find 
\begin_inset Formula $\tilde{\Delta}\in\R^{\{N\times N\}}$
\end_inset

 such that 
\begin_inset Formula $\tilde{\Delta}_{ij}=\begin{cases}
\textsf{dist}_{G}(x_{i},x_{j}) & i\ne j\\
0 & \mbox{otherwise}
\end{cases}$
\end_inset


\end_layout

\begin_layout Enumerate
Let 
\begin_inset Formula $J=I-\frac{1}{N}\mathbf{1}_{\{N\times N\}}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Construct the kernel 
\begin_inset Formula $K=-\frac{1}{2}J\{\tilde{\Delta}\}^{2}J$
\end_inset

, where 
\begin_inset Formula $\{\tilde{\Delta}\}^{2}$
\end_inset

 is the element-wise square of 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\tilde{\Delta}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Decompose into 
\begin_inset Formula $K=V\Lambda V^{-1}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Sort columns of 
\begin_inset Formula $V$
\end_inset

 in decreasing order based on the associated eigenvalues.
\end_layout

\begin_layout Enumerate
Let 
\begin_inset Formula $Y=V[:,1:d]$
\end_inset

.
\end_layout

\begin_layout Enumerate
Return 
\begin_inset Formula $Y$
\end_inset

.
\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Isomap
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Experiments
\end_layout

\begin_layout Subsubsection
Swiss Roll
\end_layout

\begin_layout Standard
Consider the following embeddings.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Isomap Embedding of Swiss Roll Dataset, 
\begin_inset Formula $k=5$
\end_inset

, 
\begin_inset Formula $N=1000$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Isomap Embedding of Swiss Roll Dataset, 
\begin_inset Formula $k=10$
\end_inset

, 
\begin_inset Formula $N=1000$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Isomap Embedding of Swiss Roll Dataset, 
\begin_inset Formula $k=15$
\end_inset

, 
\begin_inset Formula $N=1000$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Isomap Embedding of Swiss Roll Dataset, 
\begin_inset Formula $k=20$
\end_inset

, 
\begin_inset Formula $N=1000$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
S-Curve
\end_layout

\begin_layout Standard
Consider the following embeddings.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Isomap Embedding of S-Curve Dataset, 
\begin_inset Formula $k=5$
\end_inset

, 
\begin_inset Formula $N=1000$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Isomap Embedding of S-Curve Dataset, 
\begin_inset Formula $k=10$
\end_inset

, 
\begin_inset Formula $N=1000$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Isomap Embedding of S-Curve Dataset, 
\begin_inset Formula $k=15$
\end_inset

, 
\begin_inset Formula $N=1000$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Isomap Embedding of S-Curve Dataset, 
\begin_inset Formula $k=20$
\end_inset

, 
\begin_inset Formula $N=1000$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Frey Faces
\end_layout

\begin_layout Standard
Consider the following embeddings.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Isomap Embedding of Frey Faces Dataset, 
\begin_inset Formula $k=5$
\end_inset

, 
\begin_inset Formula $N=1000$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Isomap Embedding of Frey Faces Dataset, 
\begin_inset Formula $k=10$
\end_inset

, 
\begin_inset Formula $N=1000$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Isomap Embedding of Frey Faces Dataset, 
\begin_inset Formula $k=15$
\end_inset

, 
\begin_inset Formula $N=1000$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Isomap Embedding of Frey Faces Dataset, 
\begin_inset Formula $k=20$
\end_inset

, 
\begin_inset Formula $N=1000$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Laplacian Eigenmaps 
\end_layout

\begin_layout Subsection
Description
\end_layout

\begin_layout Subsection
Implementation
\end_layout

\begin_layout Subsection
Experiments
\end_layout

\begin_layout Section
Diffusion Maps 
\end_layout

\begin_layout Subsection
Description
\end_layout

\begin_layout Subsection
Implementation
\end_layout

\begin_layout Subsection
Experiments
\end_layout

\begin_layout Section
Multidimensional Scaling with Boxcox Stress Functions
\end_layout

\begin_layout Subsection
Description
\begin_inset CommandInset label
LatexCommand label
name "sub:LMDSDescription"

\end_inset


\end_layout

\begin_layout Subsubsection
Global Stress Functions
\begin_inset CommandInset label
LatexCommand label
name "sub:Global-Stress-Functions"

\end_inset


\end_layout

\begin_layout Standard
We can look at the optimization problem outlined in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:Metric-Multidimensional-Scaling"

\end_inset

 as fitting within a larger family of objective functions for determining
 a lower dimensional embedding of a dataset.
 We can consider the optimization problem once more, and write the objective
 function in terms of 
\begin_inset Quotes eld
\end_inset

attractive
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

repulsive
\begin_inset Quotes erd
\end_inset

 forces.
 Borrowing this notion from graph theory, we can think of an embedding in
 a very physical sense, essentially trying to find the energy equilibrium
 of a system of objects -- in this case, high dimensional datapoints.
 In the metric multidimensional scaling case, we have that our embedding
 can be written as a
\begin_inset Formula 
\begin{eqnarray*}
\{y_{i}\}_{i=1}^{N} & = & \underset{\{a_{i}\}_{i=1}^{N}\subset\R^{d}}{\text{arg min}}\sum_{i=1}^{n}\sum_{j=1}^{n}(\Delta_{ij}-\Vert a_{i}-a_{j}\Vert)^{2}\mbox{, but by expanding, we have}\\
 & = & \underset{\{a_{i}\}_{i=1}^{N}\subset\R^{d}}{\text{arg min}}\sum_{i=1}^{n}\sum_{j=1}^{n}\left(\underbrace{\Vert a_{i}-a_{j}\Vert{}^{2}}_{\text{min at }a_{i}=a_{j}}\overbrace{-2\Vert a_{i}-a_{j}\Vert\Delta_{ij}}^{\text{min at }\Vert a_{i}-a_{j}\Vert=\infty}\right)+\mbox{constant}.
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
As we can see, the 
\begin_inset Formula $\Vert a_{i}-a_{j}\Vert{}^{2}$
\end_inset

 term acts as an attractive force, since a minimal solution for just this
 term will want all pairs 
\begin_inset Formula $a_{i}$
\end_inset

 and 
\begin_inset Formula $a_{j}$
\end_inset

 to be identical.
 Conversely, we can see that the 
\begin_inset Formula $-2\Vert a_{i}-a_{j}\Vert\Delta_{ij}$
\end_inset

 term has the opposite effect -- a minimial solution considering only this
 term will push points as far apart as possible.
 We call such an energy function a 
\emph on
Stress Function
\emph default
, and the form of such a stress function 
\begin_inset Formula $S(\{y_{i}\}_{i=1}^{N}\vert\Delta)$
\end_inset

 for the metric multidimensional scaling case can be written as
\begin_inset Formula 
\begin{equation}
S\left(\{y_{i}\}_{i=1}^{N}\vert\Delta\right)=\sum_{i=1}^{n}\sum_{j=1}^{n}\Vert y_{i}-y_{j}\Vert{}^{2}-2\Vert y_{i}-y_{j}\Vert\Delta_{ij}.\label{eq:mmds_stress}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
In the direction of CHEN AND BUJA, we can consider Power Law transformations
 of the embedding distances, as is also done in Graph Drawing CITATION.
 We use the so-called Boxcox transformation of the embedding dissimilarities
 distance.
 The Boxcox transformations can be writted as follows.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\beta_{\alpha}(d)=\begin{cases}
\frac{d^{\alpha}-1}{\alpha} & \alpha\neq0\\
\log(d) & \alpha=0
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Standard
In lieu of the assumption of the euclidean metric above -- i.e., 
\begin_inset Formula $\Vert y_{i}-y_{j}\Vert$
\end_inset

, we use 
\begin_inset Formula $\rho(y_{i},y_{j})$
\end_inset

 for some 
\begin_inset Formula $\rho:\R^{m}\times\R^{m}\longrightarrow\R$
\end_inset

 that satisfies 
\begin_inset Formula $\rho(a,b)>0\mbox{ }\forall a,b\in\R^{m}$
\end_inset

.
\end_layout

\begin_layout Standard
To have other well-known stress functions CITATION fit into such a universal
 stress function, we can add a weighting to the stress for each term in
 the sum based on the provided matrix 
\begin_inset Formula $\Delta$
\end_inset

.
 Denote the 
\begin_inset Quotes eld
\end_inset

attractive
\begin_inset Quotes erd
\end_inset

 power as 
\begin_inset Formula $\mu+\lambda$
\end_inset

, and the repulsing power by 
\begin_inset Formula $\mu$
\end_inset

.
 We also impose that 
\begin_inset Formula $\lambda>0$
\end_inset

 and that 
\begin_inset Formula $|\mu|<\infty$
\end_inset

.
 We can now write the more general (still global) stress function as 
\begin_inset Formula 
\begin{equation}
S\left(\{y_{i}\}_{i=1}^{N}\vert\Delta\right)=\sum_{i=1}^{N}\sum_{j=1}^{N}\Delta_{ij}^{\nu}\left(\underbrace{\beta_{\mu+\lambda}\left(\rho(y_{i},y_{j})\right)}_{\text{attractive force}}\overbrace{-\Delta_{ij}^{\lambda}\beta_{\mu}\left(\rho(y_{i},y_{j})\right)}^{\text{repulsive force}}\right).\label{eq:mmds_boxcox}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Notice how for the case where 
\begin_inset Formula $\nu=0,\lambda=1,\mu=1$
\end_inset

, and 
\begin_inset Formula $\rho(a,b)=\Vert a-b\Vert$
\end_inset

, 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:mmds_boxcox"

\end_inset

 reduces to 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:mmds_stress"

\end_inset

.
 Notice that through the parameter 
\begin_inset Formula $\nu$
\end_inset

, we can control -- to an extent -- where the stress function will focus
 its reconstruction.
 For example, a large positive value of 
\begin_inset Formula $\nu$
\end_inset

 will make any optimization keep far points far, while a value of 
\begin_inset Formula $\nu$
\end_inset

 less than zero will mean that any optimization procedure will emphasize
 the preservation of local distances.
 We can do better than this implicit neighborhood consideration -- we can
 explicitly define neighborhoods on a dataset.
\end_layout

\begin_layout Subsubsection
Local Stress Functions
\end_layout

\begin_layout Standard
Consider a matrix 
\begin_inset Formula $\Delta$
\end_inset

 that is partially filled.
 That is, an entry 
\begin_inset Formula $\Delta_{ij}$
\end_inset

 exists if and only if points 
\begin_inset Formula $i$
\end_inset

 and 
\begin_inset Formula $j$
\end_inset

 are considered to be neighbors.
 This implicitly defines a graph on our dataset.
 Let 
\begin_inset Formula $G=(V,E)$
\end_inset

, where 
\begin_inset Formula $V=\{1,\ldots,N\}$
\end_inset

, edges 
\begin_inset Formula $(i,j)\in E$
\end_inset

 if and only if 
\begin_inset Formula $\Delta_{ij}$
\end_inset

 exists.
 If 
\begin_inset Formula $(i,j)\notin E$
\end_inset

, then we assign that pair an arbitrarily sized, large dissimilarity --
 call it 
\begin_inset Formula $\Delta_{\infty}.$
\end_inset

 We now have the sum of two sets of terms -- terms for which 
\begin_inset Formula $(i,j)\in E$
\end_inset

, and terms for which 
\begin_inset Formula $(i,j)\notin E$
\end_inset

.
 Formally, we have that 
\begin_inset Formula 
\begin{eqnarray}
S\left(\{y_{i}\}_{i=1}^{N}\vert\Delta\right) & = & \sum_{(i.j)\in E}\Delta_{ij}^{\nu}\left(\beta_{\mu+\lambda}\left(\rho(y_{i},y_{j})\right)-\Delta_{ij}^{\lambda}\beta_{\mu}\left(\rho(y_{i},y_{j})\right)\right)\label{eq:localizedstress}\\
 & + & \varphi\sum_{(i.j)\notin E}\Delta_{\infty}^{\nu}\left(\beta_{\mu+\lambda}\left(\rho(y_{i},y_{j})\right)-\Delta_{\infty}^{\lambda}\beta_{\mu}\left(\rho(y_{i},y_{j})\right)\right),\nonumber 
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
where we choose 
\begin_inset Formula $\varphi$
\end_inset

 such that 
\begin_inset Formula $\varphi\Delta_{\infty}^{\nu+\lambda}=t^{\nu+\lambda}$
\end_inset

 for some 
\begin_inset Formula $t\in\R$
\end_inset

.
 As per CITATION, as we let 
\begin_inset Formula $\Delta_{\infty}\longrightarrow\infty$
\end_inset

, we have that 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:localizedstress"

\end_inset

 reduces to 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
S\left(\{y_{i}\}_{i=1}^{N}\vert\Delta\right)=\underbrace{\sum_{(i.j)\in E}\Delta_{ij}^{\nu}\left(\underbrace{\beta_{\mu+\lambda}\left(\rho(y_{i},y_{j})\right)}_{\text{attractive force}}\overbrace{-\Delta_{ij}^{\lambda}\beta_{\mu}\left(\rho(y_{i},y_{j})\right)}^{\text{repulsive force}}\right)}_{\text{contrubution from implicit neighborhoods}}+\underbrace{t^{\nu+\lambda}\sum_{(i.j)\notin E}\beta_{\mu}\left(\rho(y_{i},y_{j})\right)}_{\text{contribution from non-neighbors}}.\label{eq:final_boxcox}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Notice that in the case where 
\begin_inset Formula $E=V\times V$
\end_inset

, 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:final_boxcox"

\end_inset

 reduces to 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:mmds_boxcox"

\end_inset

.
 CITE CHEN BUJA provide a derivation of the stress function and associated
 gradients such that the objective function is scale invariant, which means
 that optimization procedures can focus on embeddings up to expansions,
 rotations, and reflections.
\end_layout

\begin_layout Subsection
Implementation
\end_layout

\begin_layout Standard
Here, we provide an overview of the algorithm for finding an embedding within
 the framework laid out in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sub:LMDSDescription"

\end_inset

.
 In this, we provide a matrix-efficient formulation of the problem.
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "alg:boxcox"

\end_inset


\end_layout

\begin_layout Plain Layout
Input: A distance/dissimilarity matrix 
\begin_inset Formula $ $
\end_inset


\begin_inset Formula $\Delta\in\R^{N\times N}$
\end_inset

, an adjacency matrix 
\begin_inset Formula $A\in\{0,1\}^{N\times N}$
\end_inset

, and an embedding dimension 
\begin_inset Formula $d$
\end_inset

.
\end_layout

\begin_layout Plain Layout
Output: an embedding 
\begin_inset Formula $Y\in\R^{N\times d}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Form (In one loop):
\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset Formula $\Delta^{\nu}$
\end_inset

, 
\begin_inset Formula $\Delta^{\nu+\frac{1}{\lambda}}$
\end_inset

, 
\end_layout

\begin_layout Enumerate
\begin_inset Formula $A_{sym}$
\end_inset

 where 
\begin_inset Formula $\left(A_{sym}\right)_{ij}=\begin{cases}
1 & \mbox{if }\left(A^{T}A\right)_{ij}>0\\
0 & \mbox{otherwise}
\end{cases}$
\end_inset

 .
 Let 
\begin_inset Formula $G=(V,E)$
\end_inset

 be associated with this adjacency matrix.
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\overline{A}$
\end_inset

, where 
\begin_inset Formula $\left(\overline{A}\right)_{ij}=1-(A_{sym})_{ij}$
\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
Let 
\begin_inset Formula $t=\frac{\tau|E|}{|V|^{2}-|E|}\left(\mbox{median}_{(i,j)\in E}\left(\Delta_{ij}^{\nu+\frac{1}{\lambda}}\right)\right)$
\end_inset


\end_layout

\begin_layout Enumerate
Let 
\begin_inset Formula $\Omega\in\R^{N\times N}$
\end_inset

 be such that 
\begin_inset Formula $\Omega=tA_{sym}+\Delta^{\nu+\frac{1}{\lambda}}$
\end_inset

.
 This will be a repulsion matrix.
\end_layout

\begin_layout Enumerate
Let 
\begin_inset Formula $X_{0}$
\end_inset

 be the initial embedding, and let 
\begin_inset Formula $D_{0}$
\end_inset

 be the matrix of pairwise Euclidean distances, 
\begin_inset Formula $ $
\end_inset


\begin_inset Formula $k=0$
\end_inset

, 
\begin_inset Formula $s_{k}=10^{6}$
\end_inset


\end_layout

\begin_layout Enumerate
Until convergence:
\end_layout

\begin_deeper
\begin_layout Enumerate
Form the embedding dissimilarities 
\begin_inset Formula $D_{k}^{\mu-2}$
\end_inset

, with 
\begin_inset Formula $\mathsf{diag}\left(D_{k}^{\mu-2}\right)=0$
\end_inset

.
 Form 
\begin_inset Formula $D_{k}^{\mu+\frac{1}{\lambda}-2}$
\end_inset

, with 
\begin_inset Formula $\mathsf{diag}\left(D_{k}^{\mu+\frac{1}{\lambda}-2}\right)=0$
\end_inset


\begin_inset CommandInset label
LatexCommand label
name "enu:5a"

\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $M=\Delta^{\nu}\odot D_{k}^{\mu+\frac{1}{\lambda}-2}-D_{k}^{\mu-2}\odot\Omega$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $\nabla S_{k}[:,i]=\textsf{rowsum}\left(M\right)$
\end_inset

 for each column 
\begin_inset Formula $i$
\end_inset

.
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\nabla S_{k}=X_{k}\odot\nabla S_{k}-MX_{k}$
\end_inset

, 
\begin_inset Formula $\nabla S_{k}=\frac{\Vert X_{k}\Vert_{1}}{\Vert\nabla S_{k}\Vert_{1}}\nabla S_{k}$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $X_{k+1}=\mathsf{update}\left(X_{k},\nabla S_{k}\right)$
\end_inset

, let 
\begin_inset Formula $D_{k+1}$
\end_inset

 be the the pairwise distances of 
\begin_inset Formula $X_{k+1}$
\end_inset

, with 
\begin_inset Formula $\mathsf{diag}(D_{k+1})=1$
\end_inset


\end_layout

\begin_layout Enumerate
Form 
\begin_inset Formula $D_{k+1}^{\mu}$
\end_inset

, with 
\begin_inset Formula $\mathsf{diag}\left(D_{k+1}^{\mu}\right)=0$
\end_inset

, 
\begin_inset Formula $D_{k+1}^{\mu}=D_{k+1}^{\mu}-1$
\end_inset


\end_layout

\begin_layout Enumerate
Form 
\begin_inset Formula $D_{k+1}^{\mu+\frac{1}{\lambda}}$
\end_inset

, with 
\begin_inset Formula $\mathsf{diag}\left(D_{k+1}^{\mu+\frac{1}{\lambda}}\right)=0$
\end_inset


\end_layout

\begin_layout Enumerate

\size scriptsize
\begin_inset Formula $s_{k+1}=\begin{cases}
\frac{1}{\mu}\mathsf{sum}\left(\Delta^{\nu}\odot\log\left(D_{k+1}\right)\right)-\frac{1}{\mu}\mathsf{sum}\left(D_{k+1}^{\mu}\odot\Delta^{\nu+\frac{1}{\lambda}}\right)-\frac{t}{\mu}\mathsf{sum}\left(D_{k+1}^{\mu}\odot\overline{A}\right) & \mbox{if }\mu+\frac{1}{\lambda}=0\\
\frac{1}{\mu+1/\lambda}\mathsf{sum}\left(\Delta^{\nu}\odot\left(D_{k+1}^{\mu+\frac{1}{\lambda}}-1\right)\right)-\mathsf{sum}\left(\log\left(D_{k+1}\right)\odot\Delta^{\nu+\frac{1}{\lambda}}\right)-t\mathsf{sum}\left(\log\left(D_{k+1}\right)\odot\overline{A}\right) & \mbox{if }\mu=0\\
\frac{1}{\mu+1/\lambda}\mathsf{sum}\left(\Delta^{\nu}\odot\left(D_{k+1}^{\mu+\frac{1}{\lambda}}-1\right)\right)-\frac{1}{\mu}\mathsf{sum}\left(D_{k+1}^{\mu}\odot\Delta^{\nu+\frac{1}{\lambda}}\right)-\frac{t}{\mu}\mathsf{sum}\left(D_{k+1}^{\mu}\odot\overline{A}\right) & \mbox{otherwise}
\end{cases}$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $k=k+1$
\end_inset


\end_layout

\end_deeper
\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Boxcox Embedding
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Note that running times are determined in a slightly different way -- i.e.,
 we report 
\begin_inset Formula $O(\frac{1}{2}n)$
\end_inset

 in lieu of 
\begin_inset Formula $O(n)$
\end_inset

.
 We provide a variety of improvements to this routine.
 First, the calculation of 
\begin_inset Formula $D^{\nu}$
\end_inset

 and 
\begin_inset Formula $D^{\nu+\frac{1}{\lambda}}$
\end_inset

 are only performed if they correspond to non-zero elements of 
\begin_inset Formula $A^{T}A$
\end_inset

, where 
\begin_inset Formula $A$
\end_inset

 is the (not necessarily symmetric) adjacency matrix passed to the routine.
 While these matricies are constructed, we determine the logical negation
 of the symmetric adjacency matrix.
 This is all done to ensure we only have to traverse all 
\begin_inset Formula $n^{2}$
\end_inset

 elements of the adjacency matrix once.
 This precomputation allows the iterative optimization procedure to be more
 efficient.
 Thus, steps 1, 2, and 3 can be completed in 
\begin_inset Formula $O\left(\frac{n^{2}-n}{2}\right)$
\end_inset

 time, instead of 
\begin_inset Formula $O(6n^{2})$
\end_inset

 time as in the original implementation.
 In addition, we have precomputed 
\begin_inset Formula $\overline{A}$
\end_inset

, which saves computation later.
 For both implementations, step 4 is an 
\begin_inset Formula $O(n^{2}d)$
\end_inset

 operation.
\end_layout

\begin_layout Standard
We can compute the complexity associated with each iteration.
 Step 5(a) can be done with 
\begin_inset Formula $O\left(\frac{n^{2}-n}{2}\right)$
\end_inset

 operations in comparison to the original 
\begin_inset Formula $O(2n^{2})$
\end_inset

, and step 5(b) can be done in 
\begin_inset Formula $O(3n)$
\end_inset

 time instead of 
\begin_inset Formula $O(6n)$
\end_inset

 as in the original implementation.
 The formation of 
\begin_inset Formula $\nabla S_{k}$
\end_inset

 can be broken into the following steps.
 Step 5(c) takes 
\begin_inset Formula $O(n^{2}+nd)$
\end_inset

 operations, and is not done in the original implementation.
 Step 5(d) can be done in 
\begin_inset Formula $O(n^{2}d+4nd)$
\end_inset

 operations in comparison to the original 
\begin_inset Formula $O(2n^{2}d+5nd)$
\end_inset

.
 For the sake of simplicity, the update step, which leaves us with a complexity
 of 
\begin_inset Formula $O(nd)$
\end_inset

 for step 5(e).
 Finally, step 5(h) can be done in 
\begin_inset Formula $O(3n^{2})$
\end_inset

 time instead of the original 
\begin_inset Formula $O(4n^{2})$
\end_inset

.
 Thus, for dominant terms
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
Including constants
\end_layout

\end_inset

, each iteration has a complexity of 
\begin_inset Formula $O\left(n^{2}d+\frac{7}{2}n^{2}\right)$
\end_inset

, whereas the original implemenation has a complexity of 
\begin_inset Formula $O(2n^{2}d+6n^{2})$
\end_inset

.
 Normally, we say that both implementations have the same asymptotic running
 time -- 
\begin_inset Formula $O(n^{2}d)$
\end_inset

.
 However, since this implementation uses the Armadillo CITATION matrix library,
 such constants matter, and make the chaining of matrix expressions more
 time efficient than a basic R implementation.
 The profiling of speed-ups will be elaborated upon in section X.
\end_layout

\begin_layout Subsection
Optimization
\end_layout

\begin_layout Standard
We now turn our attention to step 5(e) of the The original implmentation
 of a 
\family typewriter
boxcox
\family default
 function utilized an adaptive version of gradient descent.
 That is, depending upon the direction of movement of the objective function,
 the step size was increased or decreased.
 In addition to this heuristic -- which in practice is very effective --
 we implment a backtracking line search algorithm for gradient descent via
 the Armijo Condition.
 Formally, the Armijo Condition states the following CITIATION.
 Consider a function 
\begin_inset Formula $f:\R^{n}\longrightarrow\R$
\end_inset

 restricted to a search direction 
\begin_inset Formula $p\in\R^{n}$
\end_inset

 from a point 
\begin_inset Formula $x\in\R^{n}$
\end_inset

.
 A 
\emph on
step length 
\begin_inset Formula $\alpha$
\end_inset

 
\emph default
is said to satisfy the Armijo condition if 
\begin_inset Formula 
\begin{equation}
f(x+\alpha p)\leq f(x)+\alpha\beta p^{T}\nabla f(x)\label{eq:armijo}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
for some small 
\begin_inset Formula $\beta\in(0,1)$
\end_inset

.
\end_layout

\begin_layout Standard
Intuitively, this ensures that a step size decreases our objective 
\begin_inset Formula $f$
\end_inset

 by 
\begin_inset Quotes eld
\end_inset

enough.
\begin_inset Quotes erd
\end_inset

 Though inexact, such a practice is necessary in any optimization problem
 where closed form representations of step sizes aren't known.
 We now apply this to a line search for our particular problem.
 We can assume that 
\begin_inset Formula $X$
\end_inset

 is a vectoral representation of our current embedding iterate.
 Given a search direction 
\begin_inset Formula $p_{k}$
\end_inset

, the objective function 
\begin_inset Formula $S(X_{k})$
\end_inset

, and the gradient 
\begin_inset Formula $\nabla S(X_{k})$
\end_inset

, we have the following algorithm.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:armijo"

\end_inset

.
 
\begin_inset Float algorithm
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "alg:armijo"

\end_inset

 
\end_layout

\begin_layout Plain Layout
Input: 
\begin_inset Formula $X_{k}$
\end_inset

, 
\begin_inset Formula $p_{k}$
\end_inset

,
\begin_inset Formula $S(X_{k})$
\end_inset

, 
\begin_inset Formula $\nabla S(X_{k})$
\end_inset

, 
\begin_inset Formula $\tau\in(0,1)$
\end_inset

, and 
\begin_inset Formula $\beta\in(0,1)$
\end_inset

 
\end_layout

\begin_layout Plain Layout
Output: Stepsize 
\begin_inset Formula $\alpha$
\end_inset


\end_layout

\begin_layout Enumerate
Let 
\begin_inset Formula $\alpha_{init}=\alpha^{(0)}=1$
\end_inset


\end_layout

\begin_layout Enumerate
While: 
\begin_inset Formula $S(X_{k}+\alpha^{(\ell)}p_{k})>S(X_{k})+\alpha^{(\ell)}\beta\nabla S(X_{k})^{T}p_{k}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset Formula $\alpha^{(\ell+1)}=\tau\alpha^{(\ell)}$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $\ell=\ell+1$
\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
Return: 
\begin_inset Formula $\alpha^{(\ell)}$
\end_inset

 
\begin_inset Caption

\begin_layout Plain Layout
Armijo Line Search
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In addition to refining the selection of learning parameters in standard
 gradient descent, we we also provide an implentation of the (BFGS) algorithm.
 Such a method relies on an iterative construction of the Hessian in order
 to obtain more information with respect to the behavior of the function.
 For the iterative construction of the inverse hessian, we use the Sherman
 Morrison formula.
 Our algorithm for BFGS, using 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:armijo"

\end_inset

, is as follows: 
\begin_inset Float algorithm
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "alg:bfgs"

\end_inset

 
\end_layout

\begin_layout Plain Layout
Input:
\begin_inset Formula $x_{0}$
\end_inset

, 
\begin_inset Formula $p_{k}$
\end_inset

,
\begin_inset Formula $s(x)$
\end_inset

, 
\begin_inset Formula $\nabla s(x)$
\end_inset

, 
\begin_inset Formula $\tau\in(0,1)$
\end_inset

, and 
\begin_inset Formula $\beta\in(0,1)$
\end_inset


\end_layout

\begin_layout Enumerate
Return: 
\begin_inset Formula $x_{final}$
\end_inset


\end_layout

\begin_layout Enumerate
Let 
\begin_inset Formula $H_{0}^{-1}=I$
\end_inset

 
\end_layout

\begin_layout Enumerate
\begin_inset Formula $k=0$
\end_inset


\end_layout

\begin_layout Enumerate
While: not converged
\end_layout

\begin_deeper
\begin_layout Enumerate
Search direction: 
\begin_inset Formula $p_{k}=-H_{k}^{-1}\nabla s(x_{k})$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $\alpha^{(k)}=\mathrm{armijo}(x_{k},p_{k},s(x_{k}),\nabla s(x_{k}),\tau,\beta)$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $x_{k+1}=x_{k}+\alpha^{(k)}p_{k}$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $s_{k}=\alpha^{(k)}p_{k}$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $y_{k}=\nabla s(x_{k+1})-\nabla s(x_{k})$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $H_{k+1}^{-1}=H_{k}^{-1}+\frac{(s_{k}^{T}y_{k}+y_{k}^{T}H_{k}^{-1}y_{k})s_{k}s_{k}^{T}}{(s_{k}^{T}y_{k})^{2}}+\frac{H_{k}^{-1}y_{k}s_{k}^{T}+s_{k}y_{k}^{T}H_{k}^{-1}}{s_{k}^{T}y_{k}}$
\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
Return: 
\begin_inset Formula $x_{final}$
\end_inset

 
\begin_inset Caption

\begin_layout Plain Layout
BFGS for boxcox
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
For the visualitzation of the speedup, consider the following graphic.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:su_lmds"

\end_inset

 
\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../Paper/speedup_lmds.pdf
	lyxscale 10
	width 40text%

\end_inset

 
\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Ratio of CPU time for DimReduce versus a baseline
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_body
\end_document
